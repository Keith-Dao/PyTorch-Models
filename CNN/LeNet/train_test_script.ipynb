{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import collections\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchsummary\n",
    "import torchvision\n",
    "import tqdm.notebook\n",
    "\n",
    "from traditional.lenet import LeNet5\n",
    "from traditional.manual_scheduler import ManualLRScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "dataset_location: str = \"../data\"\n",
    "batch_size: int = 256\n",
    "train_validation_split: float = 0.7\n",
    "\n",
    "# Torch\n",
    "device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Training\n",
    "epochs: int = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "Load the MNIST dataset from torchvision and apply padding and normalisation as part of the transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Pad(2),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(0.5, 0.5)    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validation_data = torchvision.datasets.MNIST(dataset_location, transform=transform, download=True)\n",
    "train_data, validation_data = torch.utils.data.random_split(train_validation_data, [train_validation_split, 1 - train_validation_split])\n",
    "test_data = torchvision.datasets.MNIST(dataset_location, train=False, transform=transform, download=True)\n",
    "\n",
    "num_classes = len(train_validation_data.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(dataset: torch.utils.data.Dataset) -> torch.utils.data.DataLoader:\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_loader = get_loader(train_data)\n",
    "validation_loader = get_loader(validation_data)\n",
    "test_loader = get_loader(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample() -> tuple[torch.Tensor, str]:\n",
    "    data = next(iter(train_loader))\n",
    "    return data[0][0].squeeze(0), train_validation_data.classes[data[1][0]]\n",
    "\n",
    "image, label = get_sample()\n",
    "print(f\"Class: {label}\")\n",
    "plt.imshow(image);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = LeNet5().to(device)\n",
    "torchsummary.summary(model, (1, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and scheduler\n",
    "learning_rates: list[float] = [5e-4, 2e-4, 1e-4, 5e-5, 1e-5]\n",
    "counts: list[int] = [2, 3, 3, 4]\n",
    "\n",
    "manual_lr_scheduler = ManualLRScheduler(learning_rates, counts)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rates[0])\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, manual_lr_scheduler.step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(\n",
    "    model: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler: torch.optim.lr_scheduler,\n",
    "    train_loader: torch.utils.data.DataLoader,\n",
    "    tqdm_description: str = \"\"\n",
    ") -> tuple[float, float]: \n",
    "    training_loss = training_accuracy = 0\n",
    "    for data, targets in tqdm.tqdm(train_loader, desc=tqdm_description, ncols=100):\n",
    "        data = data.to(device)\n",
    "        y = torch.nn.functional.one_hot(targets, num_classes)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(data).to(\"cpu\")\n",
    "        loss = LeNet5.loss(y_pred, y)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Metrics\n",
    "        training_loss += loss.item()\n",
    "        training_accuracy += (torch.argmin(y_pred, dim=1) == targets).sum().item()\n",
    "    scheduler.step()\n",
    "    return training_loss / len(train_loader.dataset), training_accuracy / len(train_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler: torch.optim.lr_scheduler,\n",
    "    train_loader: torch.utils.data.DataLoader,\n",
    "    train_history: dict[str, list[float]],\n",
    "    validation_loader: torch.utils.data.DataLoader,\n",
    "    validation_history: dict[str, list[float]],\n",
    "    epochs: int\n",
    ") -> None:\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        training_loss, training_accuracy = train_step(\n",
    "            model,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            train_loader,\n",
    "            f\"Training epoch {epoch}/{epochs}\"\n",
    "        )\n",
    "        print(f\"Loss: {training_loss}, Accuracy: {100 * training_accuracy:.2f}%\", flush=True)\n",
    "        train_history[\"loss\"].append(training_loss)\n",
    "        train_history[\"accuracy\"].append(training_accuracy)\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        #     for _ in tqdm.tqdm(train_loader, desc=f\"Validating epoch {epoch}/{epochs}\", ncols=100):\n",
    "        #         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history = collections.defaultdict(list)\n",
    "validation_history = collections.defaultdict(list)\n",
    "\n",
    "train(\n",
    "    model,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    train_loader,\n",
    "    train_history,\n",
    "    validation_loader,\n",
    "    validation_history,\n",
    "    epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_history[\"loss\"]:\n",
    "    plt.plot(range(1, epochs + 1), train_history[\"loss\"], label=\"Training\")\n",
    "if validation_history[\"loss\"]:\n",
    "    plt.plot(range(1, epochs + 1), validation_history[\"loss\"], label=\"Validation\")\n",
    "\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_history[\"accuracy\"]:\n",
    "    plt.plot(range(1, epochs + 1), train_history[\"accuracy\"], label=\"Training\")\n",
    "if validation_history[\"accuracy\"]:\n",
    "    plt.plot(range(1, epochs + 1), validation_history[\"accuracy\"], label=\"Validation\")\n",
    "\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
