{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchsummary\n",
    "import torchvision\n",
    "import tqdm.notebook\n",
    "\n",
    "from traditional.lenet import LeNet5\n",
    "from traditional.manual_scheduler import ManualLRScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "dataset_location: str = \"../data\"\n",
    "batch_size: int = 256\n",
    "train_validation_split: float = 0.7\n",
    "\n",
    "# Torch\n",
    "device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Training\n",
    "epochs: int = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "Load the MNIST dataset from torchvision and apply padding and normalisation as part of the transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Pad(2),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(0.5, 0.5)    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validation_data = torchvision.datasets.MNIST(dataset_location, transform=transform, download=True)\n",
    "train_data, validation_data = torch.utils.data.random_split(train_validation_data, [train_validation_split, 1 - train_validation_split])\n",
    "test_data = torchvision.datasets.MNIST(dataset_location, train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(dataset: torch.utils.data.Dataset) -> torch.utils.data.DataLoader:\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_loader = get_loader(train_data)\n",
    "validation_loader = get_loader(validation_data)\n",
    "test_loader = get_loader(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample() -> tuple[torch.Tensor, str]:\n",
    "    data = next(iter(train_loader))\n",
    "    return data[0][0].squeeze(0), train_validation_data.classes[data[1][0]]\n",
    "\n",
    "image, label = get_sample()\n",
    "print(f\"Class: {label}\")\n",
    "plt.imshow(image);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = LeNet5().to(device)\n",
    "torchsummary.summary(model, (1, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and scheduler\n",
    "learning_rates: list[float] = [5e-4, 2e-4, 1e-4, 5e-5, 1e-5]\n",
    "counts: list[int] = [2, 3, 3, 4]\n",
    "\n",
    "manual_lr_scheduler = ManualLRScheduler(learning_rates, counts)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rates[0])\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, manual_lr_scheduler.step)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
