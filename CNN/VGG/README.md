# VGG

Implementation of VGG-16 described in the research paper.

## MNIST

### Model Summary

|  Layer (type)  |    Output Shape     |   Param #   |
| :------------: | :-----------------: | :---------: |
| 2D convolution | [-1, 64, 224, 224]  |    1,792    |
|      ReLU      | [-1, 64, 224, 224]  |      0      |
| 2D convolution | [-1, 64, 224, 224]  |   36,928    |
|      ReLU      | [-1, 64, 224, 224]  |      0      |
| 2D max pooling | [-1, 64, 112, 112]  |      0      |
| 2D convolution | [-1, 128, 112, 112] |   73,856    |
|      ReLU      | [-1, 128, 112, 112] |      0      |
| 2D convolution | [-1, 128, 112, 112] |   147,584   |
|      ReLU      | [-1, 128, 112, 112] |      0      |
| 2D max pooling |  [-1, 128, 56, 56]  |      0      |
| 2D convolution |  [-1, 256, 56, 56]  |   295,168   |
|      ReLU      |  [-1, 256, 56, 56]  |      0      |
| 2D convolution |  [-1, 256, 56, 56]  |   590,080   |
|      ReLU      |  [-1, 256, 56, 56]  |      0      |
| 2D convolution |  [-1, 256, 56, 56]  |   590,080   |
|      ReLU      |  [-1, 256, 56, 56]  |      0      |
| 2D max pooling |  [-1, 256, 28, 28]  |      0      |
| 2D convolution |  [-1, 512, 28, 28]  |  1,180,160  |
|      ReLU      |  [-1, 512, 28, 28]  |      0      |
| 2D convolution |  [-1, 512, 28, 28]  |  2,359,808  |
|      ReLU      |  [-1, 512, 28, 28]  |      0      |
| 2D convolution |  [-1, 512, 28, 28]  |  2,359,808  |
|      ReLU      |  [-1, 512, 28, 28]  |      0      |
| 2D max pooling |  [-1, 512, 14, 14]  |      0      |
| 2D convolution |  [-1, 512, 14, 14]  |  2,359,808  |
|      ReLU      |  [-1, 512, 14, 14]  |      0      |
| 2D convolution |  [-1, 512, 14, 14]  |  2,359,808  |
|      ReLU      |  [-1, 512, 14, 14]  |      0      |
| 2D convolution |  [-1, 512, 14, 14]  |  2,359,808  |
|      ReLU      |  [-1, 512, 14, 14]  |      0      |
| 2D max pooling |   [-1, 512, 7, 7]   |      0      |
|    Flatten     |     [-1, 25088]     |      0      |
|    Dropout     |     [-1, 25088]     |      0      |
|     Linear     |     [-1, 4096]      | 102,764,544 |
|      ReLU      |     [-1, 4096]      |      0      |
|    Dropout     |     [-1, 4096]      |      0      |
|     Linear     |     [-1, 4096]      | 16,781,312  |
|      ReLU      |     [-1, 4096]      |      0      |
|    Dropout     |     [-1, 4096]      |      0      |
|     Linear     |      [-1, 10]       |   40,970    |

|                      |             |
| -------------------- | ----------- |
| Total params         | 134,301,514 |
| Trainable params     | 134,301,514 |
| Non-trainable params | 0           |

### Results

Training over 10 epochs with a learning rate of 1e-4 and weight decay of 1e-12.

|                     Loss                      |                       Accuracy                        |
| :-------------------------------------------: | :---------------------------------------------------: |
| ![MNIST loss graph](Resources/mnist_loss.png) | ![MNIST accuracy graph](Resources/mnist_accuracy.png) |

|              | Training | Validation | Testing |
| :----------: | :------: | :--------: | :-----: |
|     Loss     |  0.0153  |   0.0333   | 0.0267  |
| Accuracy (%) |  99.50   |   99.08    |  99.22  |

| Class | Training Precision | Validation Precision | Testing Precision | Training Recall | Validation Recall | Testing Recall | Training F1 Score | Validation F1 Score | Testing F1 Score |
| :---: | :----------------: | :------------------: | :---------------: | :-------------: | :---------------: | :------------: | :---------------: | :-----------------: | :--------------: |
|   0   |       0.9979       |        0.9971        |      0.9959       |     0.9983      |      0.9965       |     0.9980     |      0.9981       |       0.9968        |      0.9969      |
|   1   |       0.9970       |        0.9961        |      0.9991       |     0.9962      |      0.9912       |     0.9877     |      0.9966       |       0.9936        |      0.9934      |
|   2   |       0.9954       |        0.9838        |      0.9819       |     0.9961      |      0.9951       |     0.9971     |      0.9958       |       0.9894        |      0.9894      |
|   3   |       0.9965       |        0.9973        |      0.9950       |     0.9963      |      0.9870       |     0.9941     |      0.9964       |       0.9921        |      0.9946      |
|   4   |       0.9944       |        0.9937        |      0.9949       |     0.9936      |      0.9892       |     0.9929     |      0.9940       |       0.9915        |      0.9939      |
|   5   |       0.9945       |        0.9901        |      0.9922       |     0.9950      |      0.9876       |     0.9966     |      0.9947       |       0.9888        |      0.9944      |
|   6   |       0.9957       |        0.9851        |      0.9845       |     0.9959      |      0.9983       |     0.9948     |      0.9958       |       0.9917        |      0.9896      |
|   7   |       0.9927       |        0.9956        |      0.9961       |     0.9939      |      0.9801       |     0.9835     |      0.9933       |       0.9878        |      0.9897      |
|   8   |       0.9949       |        0.9920        |      0.9959       |     0.9937      |      0.9908       |     0.9877     |      0.9943       |       0.9914        |      0.9918      |
|   9   |       0.9913       |        0.9768        |      0.9862       |     0.9913      |      0.9923       |     0.9911     |      0.9913       |       0.9845        |      0.9886      |

## References

Research paper: https://arxiv.org/pdf/1409.1556.pdf
