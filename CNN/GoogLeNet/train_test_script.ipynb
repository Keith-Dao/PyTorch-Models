{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import collections\n",
    "from typing import Callable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchmetrics\n",
    "import torchsummary\n",
    "import torchvision\n",
    "import tqdm.notebook\n",
    "\n",
    "from model.googlenet import GoogLeNet\n",
    "from utils.augments import AddGaussianNoise, Clip\n",
    "from utils.dataset import get_loader, sample_first\n",
    "from utils.metrics import plot_metric, pretty_print_metrics\n",
    "from utils.train_validation import train, validate_one_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "dataset_location: str = \"../data\"\n",
    "\n",
    "# Torch\n",
    "device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(0.1307, 0.3015),\n",
    "    torchvision.transforms.Resize((224, 224), antialias=True),\n",
    "    torchvision.transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validation_split = 0.7\n",
    "\n",
    "mnist_train_validation_data = torchvision.datasets.MNIST(\n",
    "    dataset_location,\n",
    "    transform=mnist_transform,\n",
    "    download=True\n",
    ")\n",
    "mnist_train_data, mnist_validation_data = torch.utils.data.random_split(\n",
    "    mnist_train_validation_data,\n",
    "    [train_validation_split, 1 - train_validation_split]\n",
    ")\n",
    "mnist_test_data = torchvision.datasets.MNIST(\n",
    "    dataset_location,\n",
    "    train=False,\n",
    "    transform=mnist_transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "num_classes = len(mnist_train_validation_data.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size: int = 128\n",
    "mnist_train_loader = get_loader(mnist_train_data, batch_size)\n",
    "mnist_validation_loader = get_loader(mnist_validation_data, batch_size)\n",
    "mnist_test_loader = get_loader(mnist_test_data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = sample_first(mnist_train_loader, mnist_train_validation_data.classes)\n",
    "\n",
    "print(f\"Class: {label}\")\n",
    "image = torch.clamp(image.permute(1, 2, 0) * 0.3015 + 0.1307, 0, 1) # Convert to visible image\n",
    "\n",
    "plt.imshow(image);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "mnist_model = GoogLeNet(num_classes).to(device)\n",
    "torchsummary.summary(mnist_model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "mnist_epochs = 20\n",
    "auxiliary_loss_weight = 0.3\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(mnist_model.parameters(), 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history = collections.defaultdict(list)\n",
    "validation_history = collections.defaultdict(list)\n",
    "\n",
    "train_metrics = torchmetrics.MetricCollection({\n",
    "    \"accuracy\": torchmetrics.classification.MulticlassAccuracy(num_classes, average=\"micro\"),\n",
    "    \"precision\": torchmetrics.classification.MulticlassPrecision(num_classes, average=None),\n",
    "    \"recall\": torchmetrics.classification.MulticlassRecall(num_classes, average=None),\n",
    "    \"f1 score\": torchmetrics.classification.MulticlassF1Score(num_classes, average=None),\n",
    "}).to(device)\n",
    "validation_metrics = train_metrics.clone()\n",
    "\n",
    "train(\n",
    "    mnist_model,\n",
    "    optimizer,\n",
    "    None,\n",
    "    mnist_train_loader,\n",
    "    train_history,\n",
    "    mnist_validation_loader,\n",
    "    validation_history,\n",
    "    mnist_epochs,\n",
    "    loss_fn, \n",
    "    mnist_train_validation_data.classes, \n",
    "    train_metrics,\n",
    "    validation_metrics,\n",
    "    device,\n",
    "    auxiliary_loss_weight\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric({\"Training\": train_history, \"Validation\": validation_history}, metric=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric({\"Training\": train_history, \"Validation\": validation_history}, metric=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = train_metrics.clone()\n",
    "\n",
    "test_loss = validate_one_epoch(\n",
    "    mnist_model, \n",
    "    mnist_test_loader,\n",
    "    loss_fn,\n",
    "    num_classes,\n",
    "    test_metrics,\n",
    "    device,\n",
    "    \"Testing\"    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_history = {\n",
    "    \"loss\": [test_loss]\n",
    "} | {metric: [history.to(\"cpu\")] for metric, history in test_metrics.compute().items()}\n",
    "\n",
    "pretty_print_metrics(test_history, mnist_train_validation_data.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize([0.5071, 0.4865, 0.4409], [0.2009, 0.1984, 0.2023]),\n",
    "    torchvision.transforms.Resize((224, 224), antialias=True),\n",
    "])\n",
    "\n",
    "cifar_transforms_with_augmentations = torchvision.transforms.Compose([\n",
    "    cifar_transforms,\n",
    "    torchvision.transforms.RandomResizedCrop(224, antialias=True),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    AddGaussianNoise(0, 0.01),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validation_split = 0.7\n",
    "\n",
    "cifar_train_validation_data = torchvision.datasets.CIFAR100(\n",
    "    dataset_location, \n",
    "    transform=cifar_transforms,\n",
    "    download=True\n",
    ")\n",
    "cifar_train_data, cifar_validation_data = torch.utils.data.random_split(\n",
    "    cifar_train_validation_data,\n",
    "    [train_validation_split, 1 - train_validation_split]\n",
    ")\n",
    "cifar_train_data.dataset.transform = cifar_transforms_with_augmentations\n",
    "cifar_test_data = torchvision.datasets.CIFAR100(\n",
    "    dataset_location,\n",
    "    False,\n",
    "    transform=cifar_transforms,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "num_classes = len(cifar_train_validation_data.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size: int = 64\n",
    "cifar_train_loader = get_loader(cifar_train_data, batch_size)\n",
    "cifar_validation_loader = get_loader(cifar_validation_data, batch_size)\n",
    "cifar_test_loader = get_loader(cifar_test_data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = sample_first(\n",
    "    cifar_train_loader,\n",
    "    cifar_train_validation_data.classes\n",
    ")\n",
    "\n",
    "print(f\"Class: {label}\")\n",
    "image = torch.clamp(\n",
    "        image.permute(1, 2, 0) \n",
    "        * torch.tensor([0.2009, 0.1984, 0.2023]) \n",
    "        + torch.tensor([0.5071, 0.4865, 0.4409]),\n",
    "        0,\n",
    "        1\n",
    "    ) # Convert to visible image\n",
    "\n",
    "plt.imshow(image);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "cifar_model = GoogLeNet(num_classes).to(device)\n",
    "torchsummary.summary(cifar_model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "cifar_epochs = 100\n",
    "auxiliary_loss_weight = 0.3\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(cifar_model.parameters(), 1e-4, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history = collections.defaultdict(list)\n",
    "validation_history = collections.defaultdict(list)\n",
    "\n",
    "train_metrics = torchmetrics.MetricCollection({\n",
    "    \"accuracy\": torchmetrics.classification.MulticlassAccuracy(num_classes, average=\"micro\"),\n",
    "    \"precision\": torchmetrics.classification.MulticlassPrecision(num_classes, average=None),\n",
    "    \"recall\": torchmetrics.classification.MulticlassRecall(num_classes, average=None),\n",
    "    \"f1 score\": torchmetrics.classification.MulticlassF1Score(num_classes, average=None),\n",
    "}).to(device)\n",
    "validation_metrics = train_metrics.clone()\n",
    "\n",
    "train(\n",
    "    cifar_model,\n",
    "    optimizer,\n",
    "    None,\n",
    "    cifar_train_loader,\n",
    "    train_history,\n",
    "    cifar_validation_loader,\n",
    "    validation_history,\n",
    "    cifar_epochs,\n",
    "    loss_fn, \n",
    "    cifar_train_validation_data.classes, \n",
    "    train_metrics,\n",
    "    validation_metrics,\n",
    "    device,\n",
    "    auxiliary_loss_weight\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric({\"Training\": train_history, \"Validation\": validation_history}, metric=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric({\"Training\": train_history, \"Validation\": validation_history}, metric=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = train_metrics.clone()\n",
    "\n",
    "test_loss = validate_one_epoch(\n",
    "    cifar_model, \n",
    "    cifar_test_loader,\n",
    "    loss_fn,\n",
    "    num_classes,\n",
    "    test_metrics,\n",
    "    device,\n",
    "    \"Testing\"    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_history = {\n",
    "    \"loss\": [test_loss]\n",
    "} | {metric: [history.to(\"cpu\")] for metric, history in test_metrics.compute().items()}\n",
    "\n",
    "pretty_print_metrics(test_history, cifar_train_validation_data.classes)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
